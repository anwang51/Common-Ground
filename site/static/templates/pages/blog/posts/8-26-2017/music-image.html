<blog-post category="vm.currentPost.category" date="vm.currentPost.date"  name="vm.currentPost.title"  author="vm.currentPost.author"  tags="vm.currentPost.tags" date-path="vm.currentPost.datePath" title-path="vm.currentPost.titlePath">
    <p>Music genre classification is a classic problem in which we try to identify the genre of a given piece of music. It’s a challenging task in the field of <strong>Music Information Retrieval</strong> with some pretty cool applications. For example, Pandora uses genre classifications to dynamically generate images that complement the music. But how does such a classification system work?</p>
    <p>A basic idea is to take a sampled time domain spectrum at particular time steps and feed it in as data for each music sample. Here’s an image of one such time domain sample:</p>
    <img class="blog-post-image" ng-src="templates/pages/blog/posts/{{vm.currentPost.datePath}}/spectrum.png" alt="Music Spectrum in Time Domain"/>
    <p>However, in practice, this turns out to lead to poor classification performance. This is because there are no particular time domain patterns that correlate to particular genres. In other words, there’s a lot of noise and not a lot of signal. So we have to think of something a little more clever. </p>
    <p>One idea that’s used a lot in literature that involves audio and signal processing is the <strong>Mel Frequency Cepstral Coefficient </strong> (MFCC). The MFCC metric is used all the time in speech recognition applications as well.</p>
    <p>Here’s an outline of the steps of obtaining the MFCC features of an audio track:</p>
    <img class="blog-post-image" ng-src="templates/pages/blog/posts/{{vm.currentPost.datePath}}/MFCC.png" style="width: auto; height: 300" alt="MFCC Steps"/>
    <p>A lot of these steps require knowledge of signal processing techniques, but on a high level, what we are doing through this transformation is taking a time domain signal and compressing each block of this signal into a series of transformed frequency coefficients that represent a large part of the information in the audio. And one advantage of this representation is that these features tend to perform much better in applications of music information retrieval and speech recognition.</p>
    <p>Once we have these features for each music sample, we can then pass them through a machine learning algorithm as a training set and then evaluate on a test set of data. A dataset that was compiled for solving this particular problem is GTZAN Genre Collection, of 1000 audio tracks each 30 seconds long. There are 10 genres represented, each containing 100 tracks. For this project, we focused on three particular genres: classical, country, and metal music. After testing our classifier, we had an accuracy of about 89% on the test set. You can see in the video demo some of the different types of songs it can differentiate between. 
	</p>
	<p>A second part to this project was incorporating an ability to gain information from images. A lot of this relies on image classification and image description. This had been a challenging problem in deep learning for a very long time until some of the breakthroughs of the <strong> convolutional neural network </strong>, which is the model we used to generate our results.</p>
	<p>To understand how exactly the <strong> convolutional neural network </strong> (CNN) works, we have to take a look at the most fundamental image classification problem, which involves recognizing handwritten digits. Turns out that a traditional deep neural network is particularly good at identifying these digits if they are all centered such as the following example:</p>
	<img class="blog-post-image" ng-src="templates/pages/blog/posts/{{vm.currentPost.datePath}}/centered.png" alt="Successful Examples"/>
	<p>But change the locations of the writing just a little bit and the computer becomes easily confused.</p>
	<img class="blog-post-image" ng-src="templates/pages/blog/posts/{{vm.currentPost.datePath}}/not_centered.png" alt="Hard Examples"/>
	<p> <strong> Convolution </strong> is an incredibly important method that helps understand structure in a particular image. As humans, we see pictures and immediately recognize an underlying structure. For example, in the below picture, we see a child on a dinosaur outside. We notice other aspects of the picture such as the grass, toys, and emotions this image evokes without even thinking. Convolution helps computers gain a similar ability.</p>
	<img class="blog-post-image" ng-src="templates/pages/blog/posts/{{vm.currentPost.datePath}}/Child.png"/>
	<p>The first step in convolution is to split our image into a bunch of tiled blocks almost as if a small window went through and took snapshots of parts of the image. Here’s what our image looks like after this step:</p>
	<img class="blog-post-image" ng-src="templates/pages/blog/posts/{{vm.currentPost.datePath}}/Child_convoluted.png" alt="Image after First Step"/>
	The next step involves feeding each of these small image windows into a fully connected neural network which gives some output. We’ll use this output to mark if there’s any interesting attributes in these image chunks. We now have a series of outputs:
	<img class="blog-post-image" ng-src="templates/pages/blog/posts/{{vm.currentPost.datePath}}/Convolution_diagram.png" alt="Diagram of Second Step"/>
	Now one issue that we must immediately deal with is that this output array is a lot of data that we have to keep in memory. To reduce the amount of information we have to store, we use a technique called <strong> max pooling </strong>. Max pooling is pretty simple in the sense that all we do is take the maximum element (intuitively, you can think of this as the most important information) in each array. The process looks like the following:
	<img class="blog-post-image" ng-src="templates/pages/blog/posts/{{vm.currentPost.datePath}}/Pooling.png" alt="Max Pooling"/>
	This final list of arrays is something we can pass through one final neural network to generate our final outputs that make predictions on new data. For example, the system we built gave the following outputs with the image of a child:
	<img class="blog-post-image" ng-src="templates/pages/blog/posts/{{vm.currentPost.datePath}}/results.png" style="width: auto; height: 300" alt="Max Pooling"/>
	As you can see, there are a lot of interesting things that CNN’s can find in images and with even more training we can tune these models to detect even more subtle characteristics in images. Going forward, we are looking to obtain more advanced information from both music and images such as types of instruments, rythym patterns, visual question answering abilities, etc. 

	<p>If you're interested in learning more about machine learning, neural networks, and autoencoders, follow us on Facebook, check out other blog posts, and join us by submitting your application <a href="http://callaunchpad.org/#/apply" target="_blank">here</a>.</p>




</blog-post>